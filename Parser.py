# Module developed by David Quispe

""" 
It is necesary to install:
1. numpy
2. pandas
3. pathlib

"""

import pandas as pd
import numpy as np
from itertools import groupby
from pathlib import Path


def get_sorted_data(input_filename, **kwargs):

    """
    Input: direction of the txt file to be parsered. The txt file has the format ###### string# ###. 
    For example: 1234567.875 Z1 654
    Return: JSON files in the following format:
                {
                    <HEADER_1> [
                        {
                            <OS_TIME_STAMP>: "string/float",
                            <VALUE>: int
                        },
                        ... additional entries ...
                    ]

                    <HEADER_2> [
                        {
                            <OS_TIME_STAMP>: "string/float",
                            <VALUE>: int
                        },
                        ... additional entries ...
                    ]

                    ... additional headers ...

                }

    The Parser file should take `num_samples` as an 
    optional parameter. If `num_samples` is specified, get_sorted_data 
    will apply a moving average filter that will take the average of the 
    previous `num_samples` number of samples per header. If `num_samples` is not 
    specified, then no moving average filter will be applied and the 
    code will run as in part1.py. As in the previous part, your output
    will be a JSON file.

                For example, in:
                {
                    "Z1": [
                            { "osTimeStamp": "242292.1146932", "value": 998 },
                            { "osTimeStamp": "342292.1148102", "value": 994 },
                            { "osTimeStamp": "4242296.999962", "value": 992 },
                            { "osTimeStamp": "2242297.000105", "value": 991 },
                            { "osTimeStamp": "1242302.0143418", "value": 996 },
                            { "osTimeStamp": "1423025.014453", "value": 999 }
                        ],
                    "Z2": [
                            { "osTimeStamp": "5242292.1153128", "value": 13608 },
                            { "osTimeStamp": "6542292.115364", "value": 13603 },
                            { "osTimeStamp": "2342297.0008519", "value": 13607 },
                            { "osTimeStamp": "31942297.000939", "value": 13605 },
                            { "osTimeStamp": "13242302.015125", "value": 13603 },
                            { "osTimeStamp": "54342302.015239", "value": 13602 },
                            { "osTimeStamp": "22682307.015094", "value": 13605 },
                            { "osTimeStamp": "87542307.0151591", "value": 13602 },
                            { "osTimeStamp": "89642311.983746", "value": 23603 }
                        ]
                }

    """

    input_filename=Path(input_filename) # use path library so it can be used in MAC and Windows

    aux0=kwargs.get("num_samples", 1)
    aux0=int(aux0)  
    print("Original num_samples: " + str(aux0))
    if (aux0 == 0) | (aux0==""):
        aux0=1

    with open(input_filename, "r") as d:
        columns = ["osTimeStamp", "code", "value"]
        data=pd.read_csv(d, delimiter=" ", usecols=[0,1,2], names=columns)
        data.reset_index(drop=True, inplace=True)
        print("****** File is opened ******")
        #Clean the data and create a list of unique codes
        data["code"] = data["code"].map(lambda x: x.lstrip("TS>"))
        codes=data["code"].unique()
        codes=list(codes)
        print("**** Unique codes contained in the file: " + str(codes) + "****")
    # Create a dictionary that will save the final result
    response={}
    for k in codes:
        response[k]=[]
        
    # Divide data depending on code
    print("****** Data is being sorted ******")
    grouped=data.groupby("code")
    fields=["osTimeStamp", "value"]

    for name in codes:
        list1=[]
        df=grouped.get_group(name)
        df=df.drop(["code"], axis=1)

        # Validation of num_samples value for edge cases
        if aux0> len(df):
            print(str(name) + ":" + " num_samples is greater than length of raw data")
            num_samples=len(df)
        else:
            num_samples=aux0

        # Applying Average Moving Filter 
        df["avg"]=df.value.rolling(window=num_samples).mean()
        #print("Final num_sample: " + str(num_samples))
        num_samples=0 #Reset num_samples
        df_red=df.dropna(axis=0, subset=['avg'])  # Delete NA values generated by the MAF
        df_red=df_red.drop(["value"], axis=1) # Delete column 'value'

        # Create a dictionary with pairs "osTimeStamp" and "value"
        for line in range (len(df_red)):
            description=[]
            aux1=str(df_red["osTimeStamp"].iloc[line])
            aux2=int(round(df_red["avg"].iloc[line])) # rounded the average value
            description.append(aux1)
            description.append(aux2)
            i=0
            dict1={}
            while i<len(fields):
                dict1[fields[i]]=description[i]
                i=i+1
        #Create a list to save the pair dictionary
            list1.append(dict1)
        #Save the result
        response[name]=list1
    print("****** The json file has been saved ******")
    return(response)
   